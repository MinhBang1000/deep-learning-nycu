{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create NN without any techniques likes: L1, L2, ...\n",
    "    This file is the first step to take a look with NN Classification for this homework\n",
    "'''\n",
    "# Import everything we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brado\\AppData\\Local\\Temp\\ipykernel_15488\\3883930192.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data.iloc[:, 34] = data.iloc[:, 34].replace({'g': 1, 'b': 0})\n"
     ]
    }
   ],
   "source": [
    "# 1. Dataset uploading and preprocessing\n",
    "\n",
    "data = pd.read_csv(\"./Resources/2024_ionosphere_data.csv\", header=None)\n",
    "data.iloc[:, 34] = data.iloc[:, 34].replace({'g': 1, 'b': 0})\n",
    "# shuffling data\n",
    "shuffled_data = data.sample(frac=1).astype(float)\n",
    "\n",
    "# dividing data into training and test samples\n",
    "train_data = shuffled_data.iloc[:int(0.8 * len(data))]\n",
    "test_data = shuffled_data.iloc[int(0.8 * len(data)):]\n",
    "\n",
    "# highlighting features and target variable\n",
    "X_train = train_data.drop(train_data.columns[34], axis=1).values\n",
    "y_train = train_data[train_data.columns[34]].values.reshape(-1,1)\n",
    "X_test = test_data.drop(train_data.columns[34], axis=1).values\n",
    "y_test = test_data[train_data.columns[34]].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X: float64\n",
      "Data type of Y: int32\n",
      "Training set shape: (280, 34) (280, 1)\n",
      "Testing set shape: (71, 34) (71, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brado\\AppData\\Local\\Temp\\ipykernel_15488\\3223420353.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  loaded_data.iloc[:, 34] = loaded_data.iloc[:, 34].replace({\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load data\n",
    "loaded_data = pd.read_csv(\"./Resources/2024_ionosphere_data.csv\", header=None)\n",
    "\n",
    "# Convert the 'g'/'b' labels to numeric (1 for 'g' and 0 for 'b')\n",
    "loaded_data.iloc[:, 34] = loaded_data.iloc[:, 34].replace({\n",
    "    \"g\": 1,\n",
    "    \"b\": 0\n",
    "}).astype(int)  # Ensure conversion to integers\n",
    "\n",
    "# Split X and Y (Change to numpy type)\n",
    "X = loaded_data.iloc[:, 0:34].values  # Features\n",
    "Y = loaded_data.iloc[:, 34].astype(int).to_numpy()  # Labels\n",
    "\n",
    "# Check the data types\n",
    "print(\"Data type of X:\", X.dtype)\n",
    "print(\"Data type of Y:\", Y.dtype)\n",
    "\n",
    "# Split the dataset into training and testing sets using train_test_split with shuffling\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape Y to be a column vector (optional if needed)\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "\n",
    "# Verify shapes and data types\n",
    "print(\"Training set shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(type(Y_train))\n",
    "print(type(X_train))\n",
    "print(X_train.dtype)\n",
    "print(Y_train.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X: float64\n",
      "Data type of Y: int32\n",
      "351\n",
      "Training set shape: (281, 34) (281, 1)\n",
      "Testing set shape: (70, 34) (70, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brado\\AppData\\Local\\Temp\\ipykernel_15488\\3980497203.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  shuffled_data.iloc[:, 34] = shuffled_data.iloc[:, 34].replace({\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data\n",
    "loaded_data = pd.read_csv(\"./Resources/2024_ionosphere_data.csv\", header=None)\n",
    "\n",
    "# Shuffle the dataset\n",
    "# shuffled_data = loaded_data.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_data = loaded_data\n",
    "\n",
    "# Convert the 'g'/'b' labels to numeric (1 for 'g' and 0 for 'b')\n",
    "shuffled_data.iloc[:, 34] = shuffled_data.iloc[:, 34].replace({\n",
    "    \"g\": 1,\n",
    "    \"b\": 0\n",
    "}).astype(int)  # Ensure conversion to integers\n",
    "\n",
    "# Split X and Y (Change to numpy type)\n",
    "X = shuffled_data.iloc[:, 0:34].values  \n",
    "Y = shuffled_data.iloc[:, 34].astype(int).to_numpy()\n",
    "\n",
    "# Check the data types\n",
    "print(\"Data type of X:\", X.dtype)\n",
    "print(\"Data type of Y:\", Y.dtype) \n",
    "\n",
    "# Now split the dataset into training and testing sets\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "test_size = int(0.2 * len(X))\n",
    "train_size = len(X) - test_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "Y_train = Y[:train_size].reshape(-1,1)\n",
    "X_test = X[train_size:]\n",
    "Y_test = Y[train_size:].reshape(-1,1)\n",
    "\n",
    "# Verify shapes and data types\n",
    "print(\"Training set shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(type(Y_train))\n",
    "print(type(X_train))\n",
    "print(X_train.dtype)\n",
    "print(Y_train.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define necessary functions - DROP OUT\n",
    "def initialize_weights(layer_dims, seed=None):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "\n",
    "    # Setup seed for reproducibility\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Loop through the layers and initialize weights and biases using Xavier Initialization\n",
    "    for i in range(1, len(layer_dims)):  # Start from layer 1 to layer_dims-1\n",
    "        # Xavier initialization for weights\n",
    "        weights[f\"W{i}\"] = np.random.randn(layer_dims[i-1], layer_dims[i]).astype(np.float64) * np.sqrt(1 / layer_dims[i-1])\n",
    "        biases[f\"b{i}\"] = np.zeros((1, layer_dims[i]), dtype=np.float64)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def binary_cross_entropy_loss(y_true, y_pred, weights, lambda_val=0.01):\n",
    "    epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip to prevent log(0)\n",
    "\n",
    "    # Standard binary cross-entropy loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    # L2 regularization term\n",
    "    L2_regularization = 0\n",
    "    for l in range(1, len(weights) + 1):\n",
    "        L2_regularization += np.sum(np.square(weights[f\"W{l}\"]))\n",
    "\n",
    "    # Add L2 regularization to the loss\n",
    "    loss += (lambda_val / (2 * len(y_true))) * L2_regularization\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip to avoid overflow errors\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Sigmoid derivative (used in backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Derivative of sigmoid\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Tanh derivative (used in backpropagation)\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# ReLU derivative (for backpropagation)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def forward_propagation(X, weights, biases, keep_prob=1.0, is_training=True):\n",
    "    Xi = X  # Input layer\n",
    "    Z = {}\n",
    "    A = {}\n",
    "    D = {}  # Dropout masks\n",
    "    L = len(weights)  # Number of layers\n",
    "\n",
    "    for index in range(1, L + 1):  # Loop over layers from 1 to L\n",
    "        W = weights[f\"W{index}\"]\n",
    "        b = biases[f\"b{index}\"]\n",
    "        Zi = np.dot(Xi, W) + b\n",
    "        Z[f\"Z{index}\"] = Zi\n",
    "\n",
    "        # Apply Sigmoid activation for the last layer, Tanh/Relu for hidden layers\n",
    "        if index == L:\n",
    "            Ai = sigmoid(Zi)\n",
    "        else:\n",
    "            Ai = tanh(Zi)\n",
    "            if is_training and keep_prob < 1.0:  # Apply dropout during training\n",
    "                D[f\"D{index}\"] = np.random.rand(Ai.shape[0], Ai.shape[1]) < keep_prob  # Dropout mask\n",
    "                Ai = Ai * D[f\"D{index}\"]  # Apply mask\n",
    "                Ai = Ai / keep_prob  # Scale activations to maintain expectation\n",
    "\n",
    "        A[f\"A{index}\"] = Ai\n",
    "        Xi = Ai  # Output of current layer becomes input to the next layer\n",
    "\n",
    "    return Z, A, D\n",
    "\n",
    "\n",
    "def backward_propagation(\n",
    "    X, Y, Z, A, D, weights, output_activation=\"sigmoid\", hidden_activation=\"tanh\", keep_prob=1.0\n",
    "):\n",
    "    m = X.shape[0]  # Number of examples\n",
    "    gradients = {}\n",
    "    L = len(weights)  # Number of layers with weights\n",
    "    # Step 1: Initialize the derivative for the output layer\n",
    "    A_last = A[f\"A{L}\"]  # The predicted output from the last layer\n",
    "\n",
    "    if output_activation == \"sigmoid\":\n",
    "        dA_last = A_last - Y  # Error at the output layer for sigmoid\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Currently supports sigmoid for the output layer only.\"\n",
    "        )\n",
    "\n",
    "    dA_next = dA_last\n",
    "\n",
    "    # Step 2: Backpropagation through layers (from L to 1)\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        if l == L:  # For the output layer, use dA_last directly\n",
    "            dZ = dA_last\n",
    "        else:\n",
    "            # For hidden layers, backpropagate the error\n",
    "            if hidden_activation == \"relu\":\n",
    "                # Ensure proper reshaping of dA_next and weights for matrix multiplication\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * relu_derivative(Z[f\"Z{l}\"])  # Apply derivative of activation\n",
    "            elif hidden_activation == \"sigmoid\":\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * sigmoid_derivative(A[f\"A{l}\"])  # Apply derivative of sigmoid\n",
    "            elif hidden_activation == \"tanh\":\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * tanh_derivative(Z[f\"Z{l}\"])  # Apply derivative of Tanh activation\n",
    "\n",
    "            if keep_prob < 1.0:  # Dropout applied\n",
    "                dZ = dZ * D[f\"D{l}\"]  # Apply dropout mask\n",
    "                dZ = dZ / keep_prob  # Scale the gradient to maintain expectation\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        A_prev = A[f\"A{l-1}\"] if l > 1 else X  # A_prev is the activation from the previous layer or input X\n",
    "        gradients[f\"dW{l}\"] = np.dot(A_prev.T, dZ) / m  # Gradient of W\n",
    "        gradients[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True) / m  # Gradient of b\n",
    "\n",
    "        # Store dA_next for the next iteration\n",
    "        dA_next = dZ\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(weights, biases, gradients, learning_rate, lambda_val=0.01):\n",
    "    L = len(weights)  # Number of layers\n",
    "\n",
    "    # Update each layer's weights and biases with L2 regularization\n",
    "    for l in range(1, L + 1):\n",
    "        # Apply L2 regularization for weight updates\n",
    "        weights[f\"W{l}\"] -= learning_rate * (gradients[f\"dW{l}\"] + lambda_val * weights[f\"W{l}\"])\n",
    "        biases[f\"b{l}\"] -= learning_rate * gradients[f\"db{l}\"]  # Biases do not use L2 regularization\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def save_parameters(weights, biases, index):\n",
    "    file_path = \"./Parameters\"\n",
    "    np.save(f\"{file_path}/weights_{index}.npy\", weights)\n",
    "    np.save(f\"{file_path}/biases_{index}.npy\", biases)\n",
    "\n",
    "def train(\n",
    "        X, \n",
    "        Y, \n",
    "        layer_dims, \n",
    "        epoches, \n",
    "        learning_rate, \n",
    "        train_losses, \n",
    "        X_test, \n",
    "        Y_test, \n",
    "        test_losses, \n",
    "        seed = None, \n",
    "        keep_prob=1.0, \n",
    "        lambda_val=0.01\n",
    "    ):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    weights, biases = initialize_weights(layer_dims, seed)\n",
    "    save_parameters(weights, biases, \"initialize\")\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(epoches):\n",
    "        # Forward propagation with dropout\n",
    "        Z, A, D = forward_propagation(X, weights, biases, keep_prob=keep_prob, is_training=True)\n",
    "        \n",
    "        # Calculate training loss with L2 regularization\n",
    "        train_loss = binary_cross_entropy_loss(Y, A[f\"A{len(layer_dims) - 1}\"], weights, lambda_val)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Backward propagation with dropout masks\n",
    "        gradients = backward_propagation(X, Y, Z, A, D, weights, \"sigmoid\", \"tanh\", keep_prob=keep_prob)\n",
    "        \n",
    "        # Update parameters with L2 regularization\n",
    "        weights, biases = update_parameters(weights, biases, gradients, learning_rate, lambda_val)\n",
    "        \n",
    "        # Test set forward propagation (without dropout)\n",
    "        _, A_test, _ = forward_propagation(X_test, weights, biases, keep_prob=1.0, is_training=False)\n",
    "        \n",
    "        # Calculate test loss with L2 regularization\n",
    "        test_loss = binary_cross_entropy_loss(Y_test, A_test[f\"A{len(layer_dims) - 1}\"], weights, lambda_val)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # Print epoch tracking every 500 epochs\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch {i+1}/{epoches}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "\n",
    "    return weights, biases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define necessary functions\n",
    "def initialize_weights(layer_dims, seed=None):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "\n",
    "    # Setup seed for reproducibility\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Loop through the layers and initialize weights and biases using Xavier Initialization\n",
    "    for i in range(1, len(layer_dims)):  # Start from layer 1 to layer_dims-1\n",
    "        # Xavier initialization for weights\n",
    "        weights[f\"W{i}\"] = np.random.randn(layer_dims[i-1], layer_dims[i]).astype(np.float64) * np.sqrt(1 / layer_dims[i-1])\n",
    "        biases[f\"b{i}\"] = np.zeros((1, layer_dims[i]), dtype=np.float64)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip to prevent log(0)\n",
    "\n",
    "    # Standard binary cross-entropy loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip to avoid overflow errors\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Sigmoid derivative (used in backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Derivative of sigmoid\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Tanh derivative (used in backpropagation)\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# ReLU derivative (for backpropagation)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def forward_propagation(X, weights, biases):\n",
    "    Xi = X  # Input layer\n",
    "    Z = {}\n",
    "    A = {}\n",
    "    L = len(\n",
    "        weights\n",
    "    )  # Number of layers / Just 3 weights inside --> So we need plus one for layers\n",
    "\n",
    "    for index in range(1, L + 1):  # Loop over layers from 1 to L\n",
    "        W = weights[f\"W{index}\"]\n",
    "        b = biases[f\"b{index}\"]\n",
    "        Zi = np.dot(Xi, W) + b\n",
    "        Z[f\"Z{index}\"] = Zi\n",
    "\n",
    "        # Apply Sigmoid activation for the last layer, ReLU for hidden layers\n",
    "        if index == L:\n",
    "            Ai = sigmoid(Zi)\n",
    "        else:\n",
    "            Ai = tanh(Zi)\n",
    "        A[f\"A{index}\"] = Ai\n",
    "        Xi = Ai  # Output of current layer becomes input to the next layer\n",
    "\n",
    "    return Z, A\n",
    "\n",
    "\n",
    "def backward_propagation(\n",
    "    X, Y, Z, A, weights, output_activation=\"sigmoid\", hidden_activation=\"tanh\"\n",
    "):\n",
    "    m = X.shape[0]  # Number of examples\n",
    "    gradients = {}\n",
    "    L = len(weights)  # Number of layers with weights\n",
    "    # Step 1: Initialize the derivative for the output layer\n",
    "    A_last = A[f\"A{L}\"]  # The predicted output from the last layer\n",
    "\n",
    "    if output_activation == \"sigmoid\":\n",
    "        dA_last = A_last - Y  # Error at the output layer for sigmoid\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Currently supports sigmoid for the output layer only.\"\n",
    "        )\n",
    "\n",
    "    dA_next = dA_last\n",
    "\n",
    "    # Step 2: Backpropagation through layers (from L to 1)\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        if l == L:  # For the output layer, use dA_last directly\n",
    "            dZ = dA_last\n",
    "        else:\n",
    "            # For hidden layers, backpropagate the error\n",
    "            if hidden_activation == \"relu\":\n",
    "                # Ensure proper reshaping of dA_next and weights for matrix multiplication\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * relu_derivative(\n",
    "                    Z[f\"Z{l}\"]\n",
    "                )  # Apply derivative of activation\n",
    "            elif hidden_activation == \"sigmoid\":\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * sigmoid_derivative(\n",
    "                    A[f\"A{l}\"]\n",
    "                )  # Apply derivative of sigmoid\n",
    "            elif hidden_activation == \"tanh\":  # New condition for tanh activation\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * tanh_derivative(\n",
    "                    Z[f\"Z{l}\"]\n",
    "                )  # Apply derivative of Tanh activation\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Unsupported activation function: {hidden_activation}\"\n",
    "                )\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        A_prev = (\n",
    "            A[f\"A{l-1}\"] if l > 1 else X\n",
    "        )  # A_prev is the activation from the previous layer or input X\n",
    "        gradients[f\"dW{l}\"] = np.dot(A_prev.T, dZ) / m  # Gradient of W\n",
    "        gradients[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True) / m  # Gradient of b\n",
    "\n",
    "        # Store dA_next for the next iteration\n",
    "        dA_next = dZ\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    L = len(weights)  # Number of layers\n",
    "\n",
    "    # Update each layer's weights and biases without L2 regularization\n",
    "    for l in range(1, L + 1):\n",
    "        weights[f\"W{l}\"] -= learning_rate * gradients[f\"dW{l}\"]\n",
    "        biases[f\"b{l}\"] -= learning_rate * gradients[f\"db{l}\"]\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def save_parameters(weights, biases, index):\n",
    "    file_path = \"./Parameters\"\n",
    "    np.save(f\"{file_path}/weights_{index}.npy\", weights)\n",
    "    np.save(f\"{file_path}/biases_{index}.npy\", biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply L2 - Step 2 Also\n",
    "# Step 2: Define necessary functions\n",
    "def initialize_weights(layer_dims, seed=None):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "\n",
    "    # Setup seed for reproducibility\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Loop through the layers and initialize weights and biases using Xavier Initialization\n",
    "    for i in range(1, len(layer_dims)):  # Start from layer 1 to layer_dims-1\n",
    "        # Xavier initialization for weights\n",
    "        weights[f\"W{i}\"] = np.random.randn(layer_dims[i-1], layer_dims[i]).astype(np.float64) * np.sqrt(1 / layer_dims[i-1])\n",
    "        biases[f\"b{i}\"] = np.zeros((1, layer_dims[i]), dtype=np.float64)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def binary_cross_entropy_loss(y_true, y_pred, weights, lambda_val=0.01):\n",
    "    epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip to prevent log(0)\n",
    "\n",
    "    # Standard binary cross-entropy loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    # L2 regularization term\n",
    "    L2_regularization = 0\n",
    "    for l in range(1, len(weights) + 1):\n",
    "        L2_regularization += np.sum(np.square(weights[f\"W{l}\"]))\n",
    "\n",
    "    # Add L2 regularization to the loss\n",
    "    loss += (lambda_val / (2 * len(y_true))) * L2_regularization\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip to avoid overflow errors\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Sigmoid derivative (used in backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Derivative of sigmoid\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Tanh derivative (used in backpropagation)\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# ReLU derivative (for backpropagation)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def forward_propagation(X, weights, biases):\n",
    "    Xi = X  # Input layer\n",
    "    Z = {}\n",
    "    A = {}\n",
    "    L = len(\n",
    "        weights\n",
    "    )  # Number of layers / Just 3 weights inside --> So we need plus one for layers\n",
    "\n",
    "    for index in range(1, L + 1):  # Loop over layers from 1 to L\n",
    "        W = weights[f\"W{index}\"]\n",
    "        b = biases[f\"b{index}\"]\n",
    "        Zi = np.dot(Xi, W) + b\n",
    "        Z[f\"Z{index}\"] = Zi\n",
    "\n",
    "        # Apply Sigmoid activation for the last layer, ReLU for hidden layers\n",
    "        if index == L:\n",
    "            Ai = sigmoid(Zi)\n",
    "        else:\n",
    "            Ai = tanh(Zi)\n",
    "        A[f\"A{index}\"] = Ai\n",
    "        Xi = Ai  # Output of current layer becomes input to the next layer\n",
    "\n",
    "    return Z, A\n",
    "\n",
    "\n",
    "def backward_propagation(\n",
    "    X, Y, Z, A, weights, output_activation=\"sigmoid\", hidden_activation=\"tanh\"\n",
    "):\n",
    "    m = X.shape[0]  # Number of examples\n",
    "    gradients = {}\n",
    "    L = len(weights)  # Number of layers with weights\n",
    "    # Step 1: Initialize the derivative for the output layer\n",
    "    A_last = A[f\"A{L}\"]  # The predicted output from the last layer\n",
    "\n",
    "    if output_activation == \"sigmoid\":\n",
    "        dA_last = A_last - Y  # Error at the output layer for sigmoid\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Currently supports sigmoid for the output layer only.\"\n",
    "        )\n",
    "\n",
    "    dA_next = dA_last\n",
    "\n",
    "    # Step 2: Backpropagation through layers (from L to 1)\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        if l == L:  # For the output layer, use dA_last directly\n",
    "            dZ = dA_last\n",
    "        else:\n",
    "            # For hidden layers, backpropagate the error\n",
    "            if hidden_activation == \"relu\":\n",
    "                # Ensure proper reshaping of dA_next and weights for matrix multiplication\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * relu_derivative(\n",
    "                    Z[f\"Z{l}\"]\n",
    "                )  # Apply derivative of activation\n",
    "            elif hidden_activation == \"sigmoid\":\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * sigmoid_derivative(\n",
    "                    A[f\"A{l}\"]\n",
    "                )  # Apply derivative of sigmoid\n",
    "            elif hidden_activation == \"tanh\":  # New condition for tanh activation\n",
    "                dA_next = np.dot(dA_next, weights[f\"W{l+1}\"].T)\n",
    "                dZ = dA_next * tanh_derivative(\n",
    "                    Z[f\"Z{l}\"]\n",
    "                )  # Apply derivative of Tanh activation\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Unsupported activation function: {hidden_activation}\"\n",
    "                )\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        A_prev = (\n",
    "            A[f\"A{l-1}\"] if l > 1 else X\n",
    "        )  # A_prev is the activation from the previous layer or input X\n",
    "        gradients[f\"dW{l}\"] = np.dot(A_prev.T, dZ) / m  # Gradient of W\n",
    "        gradients[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True) / m  # Gradient of b\n",
    "\n",
    "        # Store dA_next for the next iteration\n",
    "        dA_next = dZ\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(weights, biases, gradients, learning_rate, lambda_val=0.01):\n",
    "    L = len(weights)  # Number of layers\n",
    "\n",
    "    # Update each layer's weights and biases with L2 regularization\n",
    "    for l in range(1, L + 1):\n",
    "        # Apply L2 regularization for weight updates\n",
    "        weights[f\"W{l}\"] -= learning_rate * (gradients[f\"dW{l}\"] + lambda_val * weights[f\"W{l}\"])\n",
    "        biases[f\"b{l}\"] -= learning_rate * gradients[f\"db{l}\"]  # Biases do not use L2 regularization\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "\n",
    "def save_parameters(weights, biases, index):\n",
    "    file_path = \"./Parameters\"\n",
    "    np.save(f\"{file_path}/weights_{index}.npy\", weights)\n",
    "    np.save(f\"{file_path}/biases_{index}.npy\", biases)\n",
    "\n",
    "def train(\n",
    "        X, \n",
    "        Y, \n",
    "        layer_dims, \n",
    "        epoches, \n",
    "        learning_rate, \n",
    "        train_losses, \n",
    "        X_test, \n",
    "        Y_test, \n",
    "        test_losses, \n",
    "        seed = None\n",
    "    ):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    # Initialize weights and biases\n",
    "    weights, biases = initialize_weights(layer_dims, seed)\n",
    "    save_parameters(weights, biases, \"initialize\")\n",
    "    # Training loop\n",
    "    for i in range(epoches):\n",
    "        # Forward propagation\n",
    "        Z, A = forward_propagation(X, weights, biases)\n",
    "        # Calculate training loss\n",
    "        train_loss = binary_cross_entropy_loss(Y, A[f\"A{len(layer_dims) - 1}\"], weights, lambda_val=0.01)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = backward_propagation(X, Y, Z, A, weights, \"sigmoid\", \"tanh\")\n",
    "        weights, biases = update_parameters(weights, biases, gradients, learning_rate, lambda_val=0.01)\n",
    "        \n",
    "        # Test set forward propagation\n",
    "        _, A_test = forward_propagation(X_test, weights, biases)\n",
    "        \n",
    "        # Calculate test loss\n",
    "        test_loss = binary_cross_entropy_loss(Y_test, A_test[f\"A{len(layer_dims) - 1}\"], weights, lambda_val=0.01)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # Print epoch tracking every 500 epochs\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch {i+1}/{epoches}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: {'W1': array([[-0.00972875, -0.17309556, -0.0336952 , ...,  0.12465341,\n",
      "         0.10040263,  0.02691714],\n",
      "       [ 0.20523374, -0.21113598, -0.02604728, ...,  0.03478585,\n",
      "        -0.08933267, -0.07251928],\n",
      "       [-0.05739922, -0.13437617,  0.1674981 , ..., -0.29938782,\n",
      "         0.00126679,  0.17190021],\n",
      "       ...,\n",
      "       [ 0.13412945, -0.00843141,  0.05298247, ..., -0.06199949,\n",
      "        -0.09183536, -0.07571608],\n",
      "       [ 0.03908794, -0.02584269, -0.18637839, ..., -0.18849223,\n",
      "        -0.08288773,  0.11404734],\n",
      "       [-0.15359538, -0.10271168, -0.05880316, ...,  0.04639549,\n",
      "        -0.13845335, -0.08131848]]), 'W2': array([[-1.03040127e-01, -3.11088018e-02, -3.00803058e-02, ...,\n",
      "         2.19821074e-02,  1.40171784e-01,  9.42759799e-02],\n",
      "       [-2.35686613e-01,  7.95025850e-02, -5.13521779e-02, ...,\n",
      "        -1.12885160e-03,  3.45762573e-02, -1.02058302e-02],\n",
      "       [ 6.01099955e-03,  1.14135745e-01,  9.34381278e-02, ...,\n",
      "        -8.16106878e-02, -4.84126540e-05,  5.40384519e-02],\n",
      "       ...,\n",
      "       [-2.05323671e-01,  4.10646906e-03, -4.54124099e-02, ...,\n",
      "         5.11349943e-02, -1.03419144e-02,  1.10682404e-01],\n",
      "       [ 8.07955534e-02, -7.51808265e-02, -6.07868015e-02, ...,\n",
      "        -3.59241448e-03, -4.17828746e-02, -1.16857282e-01],\n",
      "       [-1.14650734e-01,  3.60695098e-02,  8.00506380e-03, ...,\n",
      "         6.61503976e-02,  6.70148133e-02,  1.51996702e-01]]), 'W3': array([[-0.24789175, -0.01575727, -0.25505936, ..., -0.18612494,\n",
      "        -0.0932597 , -0.08643516],\n",
      "       [-0.05554854, -0.1194864 ,  0.11739495, ..., -0.07993469,\n",
      "         0.02881819, -0.00363718],\n",
      "       [ 0.14900476,  0.10400908, -0.22071383, ..., -0.12772378,\n",
      "         0.0367828 ,  0.08187636],\n",
      "       ...,\n",
      "       [ 0.0422908 ,  0.11281846,  0.19173144, ...,  0.19029393,\n",
      "         0.25422083, -0.10148437],\n",
      "       [ 0.03888637, -0.03277992,  0.15878967, ..., -0.07906952,\n",
      "        -0.11672737, -0.11794574],\n",
      "       [ 0.09649379,  0.00995193,  0.02519077, ...,  0.18840853,\n",
      "        -0.03473059,  0.24837196]]), 'W4': array([[-0.04141815],\n",
      "       [-0.1226903 ],\n",
      "       [-0.00692332],\n",
      "       [ 0.0458898 ],\n",
      "       [-0.07275174],\n",
      "       [ 0.07038173],\n",
      "       [-0.02637089],\n",
      "       [ 0.22441517],\n",
      "       [ 0.08978358],\n",
      "       [ 0.00190243],\n",
      "       [-0.05595103],\n",
      "       [ 0.14192367],\n",
      "       [ 0.00550988],\n",
      "       [-0.07998705],\n",
      "       [-0.12783437],\n",
      "       [ 0.12363703],\n",
      "       [-0.0833741 ],\n",
      "       [-0.05301925],\n",
      "       [ 0.09780302],\n",
      "       [ 0.18930977],\n",
      "       [ 0.10520917],\n",
      "       [ 0.1507486 ],\n",
      "       [ 0.12543676],\n",
      "       [ 0.00146772],\n",
      "       [ 0.06975324],\n",
      "       [ 0.32277518],\n",
      "       [-0.03463705],\n",
      "       [-0.19311958],\n",
      "       [-0.44660459],\n",
      "       [ 0.34216463],\n",
      "       [ 0.35016051],\n",
      "       [-0.17516049]])}\n",
      "Biases: {'b1': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'b2': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'b3': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'b4': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of initialize function\n",
    "'''\n",
    "    You can run multiple times to see the values of weights and biases do not change.\n",
    "    We can use those values for a lot of test after we change our parameters.\n",
    "'''\n",
    "layer_dims = [64, 128, 64, 32, 1]  # Example for a network with 3 hidden layers\n",
    "weights, biases = initialize_weights(layer_dims) # Fixed the values of weights and biases\n",
    "\n",
    "# Output the initialized weights and biases (you can print to check them)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Biases:\", biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000, Train Loss: 0.8623241010210342, Test Loss: 0.8199198431601161\n",
      "Epoch 501/50000, Train Loss: 0.6618187067599051, Test Loss: 0.6335906562350739\n",
      "Epoch 1001/50000, Train Loss: 0.6455218409796539, Test Loss: 0.5972552287124687\n",
      "Epoch 1501/50000, Train Loss: 0.6153327031893666, Test Loss: 0.583988285067378\n",
      "Epoch 2001/50000, Train Loss: 0.5674552251907377, Test Loss: 0.575061116098542\n",
      "Epoch 2501/50000, Train Loss: 0.5793273111704106, Test Loss: 0.5672026674705563\n",
      "Epoch 3001/50000, Train Loss: 0.5279722611463713, Test Loss: 0.5597878171562165\n",
      "Epoch 3501/50000, Train Loss: 0.556608694198882, Test Loss: 0.5525802750307969\n",
      "Epoch 4001/50000, Train Loss: 0.5546343431595656, Test Loss: 0.5456202625289899\n",
      "Epoch 4501/50000, Train Loss: 0.5215946084330599, Test Loss: 0.5386900157254118\n",
      "Epoch 5001/50000, Train Loss: 0.5055973105411223, Test Loss: 0.5319931287548129\n",
      "Epoch 5501/50000, Train Loss: 0.5165534756147631, Test Loss: 0.5254053045296676\n",
      "Epoch 6001/50000, Train Loss: 0.5112611106670627, Test Loss: 0.5189939789139358\n",
      "Epoch 6501/50000, Train Loss: 0.5538831201057131, Test Loss: 0.5126898018916239\n",
      "Epoch 7001/50000, Train Loss: 0.4880734906872369, Test Loss: 0.506544532288933\n",
      "Epoch 7501/50000, Train Loss: 0.49450195224096244, Test Loss: 0.5005093779974261\n",
      "Epoch 8001/50000, Train Loss: 0.4772319273140132, Test Loss: 0.49463735917864765\n",
      "Epoch 8501/50000, Train Loss: 0.4819185566732533, Test Loss: 0.48889332691481135\n",
      "Epoch 9001/50000, Train Loss: 0.48816653131536164, Test Loss: 0.4835567143921422\n",
      "Epoch 9501/50000, Train Loss: 0.4643611015049937, Test Loss: 0.47833896641907314\n",
      "Epoch 10001/50000, Train Loss: 0.4539914607764162, Test Loss: 0.47342683904581856\n",
      "Epoch 10501/50000, Train Loss: 0.44480988036260655, Test Loss: 0.4686844234411618\n",
      "Epoch 11001/50000, Train Loss: 0.45187391361096035, Test Loss: 0.46420969317456723\n",
      "Epoch 11501/50000, Train Loss: 0.41201570409192384, Test Loss: 0.46002066114662676\n",
      "Epoch 12001/50000, Train Loss: 0.41345599085767676, Test Loss: 0.45621265546604617\n",
      "Epoch 12501/50000, Train Loss: 0.4526742817581512, Test Loss: 0.4527363351990026\n",
      "Epoch 13001/50000, Train Loss: 0.41780272155954024, Test Loss: 0.44941484006172\n",
      "Epoch 13501/50000, Train Loss: 0.4321807549772926, Test Loss: 0.4464145992116188\n",
      "Epoch 14001/50000, Train Loss: 0.42775414340656487, Test Loss: 0.4436861896551584\n",
      "Epoch 14501/50000, Train Loss: 0.40691771934603294, Test Loss: 0.4409752026044786\n",
      "Epoch 15001/50000, Train Loss: 0.38654415368871514, Test Loss: 0.4384304423388112\n",
      "Epoch 15501/50000, Train Loss: 0.41597423416470575, Test Loss: 0.4361876316580195\n",
      "Epoch 16001/50000, Train Loss: 0.39026085716417563, Test Loss: 0.4341264434820497\n",
      "Epoch 16501/50000, Train Loss: 0.4062809315900773, Test Loss: 0.4323192171968012\n",
      "Epoch 17001/50000, Train Loss: 0.3677165204014355, Test Loss: 0.43072308208962296\n",
      "Epoch 17501/50000, Train Loss: 0.3382843677368358, Test Loss: 0.4294010923414972\n",
      "Epoch 18001/50000, Train Loss: 0.37231830041928343, Test Loss: 0.4279724888087975\n",
      "Epoch 18501/50000, Train Loss: 0.3438322933286021, Test Loss: 0.4267740123535706\n",
      "Epoch 19001/50000, Train Loss: 0.39336159675412213, Test Loss: 0.42577696578671714\n",
      "Epoch 19501/50000, Train Loss: 0.3778986348535631, Test Loss: 0.42481590364056865\n",
      "Epoch 20001/50000, Train Loss: 0.3388370740272533, Test Loss: 0.42388294735047544\n",
      "Epoch 20501/50000, Train Loss: 0.34546732086153703, Test Loss: 0.4229288979395461\n",
      "Epoch 21001/50000, Train Loss: 0.3630562029504394, Test Loss: 0.4222006462834976\n",
      "Epoch 21501/50000, Train Loss: 0.35680469303605444, Test Loss: 0.4216434334317902\n",
      "Epoch 22001/50000, Train Loss: 0.36769736646175044, Test Loss: 0.4211398169597274\n",
      "Epoch 22501/50000, Train Loss: 0.35850360640270773, Test Loss: 0.4206085723028671\n",
      "Epoch 23001/50000, Train Loss: 0.3213416816550118, Test Loss: 0.42025934375535073\n",
      "Epoch 23501/50000, Train Loss: 0.3196873411571478, Test Loss: 0.4197428194079891\n",
      "Epoch 24001/50000, Train Loss: 0.3277369138245987, Test Loss: 0.41933996131119766\n",
      "Epoch 24501/50000, Train Loss: 0.3608310867595161, Test Loss: 0.4191999187506315\n",
      "Epoch 25001/50000, Train Loss: 0.3613674329573357, Test Loss: 0.4189200404835103\n",
      "Epoch 25501/50000, Train Loss: 0.32362047608309336, Test Loss: 0.41847915306788236\n",
      "Epoch 26001/50000, Train Loss: 0.33234681165766283, Test Loss: 0.4181032431384021\n",
      "Epoch 26501/50000, Train Loss: 0.33505319853550447, Test Loss: 0.41793080872572164\n",
      "Epoch 27001/50000, Train Loss: 0.3118691727639289, Test Loss: 0.4175852098253853\n",
      "Epoch 27501/50000, Train Loss: 0.34027769160311405, Test Loss: 0.4177175605385155\n",
      "Epoch 28001/50000, Train Loss: 0.32468330977873744, Test Loss: 0.4175757198401647\n",
      "Epoch 28501/50000, Train Loss: 0.32509242834631424, Test Loss: 0.4175299017682874\n",
      "Epoch 29001/50000, Train Loss: 0.2991574526782405, Test Loss: 0.41719903664888774\n",
      "Epoch 29501/50000, Train Loss: 0.295415388837798, Test Loss: 0.416850491039356\n",
      "Epoch 30001/50000, Train Loss: 0.31172718281906553, Test Loss: 0.4167716591954823\n",
      "Epoch 30501/50000, Train Loss: 0.3172292648266823, Test Loss: 0.4164285886310097\n",
      "Epoch 31001/50000, Train Loss: 0.31794723027333666, Test Loss: 0.41603636692573803\n",
      "Epoch 31501/50000, Train Loss: 0.3296898791644397, Test Loss: 0.41599531403383244\n",
      "Epoch 32001/50000, Train Loss: 0.29447697612758883, Test Loss: 0.4163465294683396\n",
      "Epoch 32501/50000, Train Loss: 0.29814217135486, Test Loss: 0.4163490498391994\n",
      "Epoch 33001/50000, Train Loss: 0.3001863300143441, Test Loss: 0.41604928744613734\n",
      "Epoch 33501/50000, Train Loss: 0.2939149737560051, Test Loss: 0.4155897214305504\n",
      "Epoch 34001/50000, Train Loss: 0.2967221903027429, Test Loss: 0.41574257581552254\n",
      "Epoch 34501/50000, Train Loss: 0.2986285334520545, Test Loss: 0.41608169148552526\n",
      "Epoch 35001/50000, Train Loss: 0.2826741816238487, Test Loss: 0.41548622085081016\n",
      "Epoch 35501/50000, Train Loss: 0.2833144677759902, Test Loss: 0.4157501090892356\n",
      "Epoch 36001/50000, Train Loss: 0.27851010399747744, Test Loss: 0.415879531811487\n",
      "Epoch 36501/50000, Train Loss: 0.279821304669788, Test Loss: 0.41594039669700095\n",
      "Epoch 37001/50000, Train Loss: 0.2652803919828625, Test Loss: 0.41614815049618364\n",
      "Epoch 37501/50000, Train Loss: 0.27180645199581843, Test Loss: 0.41573491577227134\n",
      "Epoch 38001/50000, Train Loss: 0.2685679269204325, Test Loss: 0.4160386771488059\n",
      "Epoch 38501/50000, Train Loss: 0.2634774886902039, Test Loss: 0.41605787010161016\n",
      "Epoch 39001/50000, Train Loss: 0.27050335232699896, Test Loss: 0.4162622303566519\n",
      "Epoch 39501/50000, Train Loss: 0.28946313977855315, Test Loss: 0.4160912985727586\n",
      "Epoch 40001/50000, Train Loss: 0.29551091506544924, Test Loss: 0.4167357089577489\n",
      "Epoch 40501/50000, Train Loss: 0.3089449078061947, Test Loss: 0.41730128677688194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m lambda_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m  \u001b[38;5;66;03m# Add L2 regularization strength\u001b[39;00m\n\u001b[0;32m      8\u001b[0m keep_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m    \u001b[38;5;66;03m# Apply dropout to hidden layers\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m weights, biases \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_losses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# For reproducibility\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_val\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m save_parameters(weights, biases, index_run)\n",
      "Cell \u001b[1;32mIn[179], line 191\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, Y, layer_dims, epoches, learning_rate, train_losses, X_test, Y_test, test_losses, seed, keep_prob, lambda_val)\u001b[0m\n\u001b[0;32m    188\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Backward propagation with dropout masks\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtanh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Update parameters with L2 regularization\u001b[39;00m\n\u001b[0;32m    194\u001b[0m weights, biases \u001b[38;5;241m=\u001b[39m update_parameters(weights, biases, gradients, learning_rate, lambda_val)\n",
      "Cell \u001b[1;32mIn[179], line 134\u001b[0m, in \u001b[0;36mbackward_propagation\u001b[1;34m(X, Y, Z, A, D, weights, output_activation, hidden_activation, keep_prob)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Compute gradients for weights and biases\u001b[39;00m\n\u001b[0;32m    133\u001b[0m A_prev \u001b[38;5;241m=\u001b[39m A[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X  \u001b[38;5;66;03m# A_prev is the activation from the previous layer or input X\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m gradients[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_prev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m m  \u001b[38;5;66;03m# Gradient of W\u001b[39;00m\n\u001b[0;32m    135\u001b[0m gradients[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m m  \u001b[38;5;66;03m# Gradient of b\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Store dA_next for the next iteration\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index_run = 1\n",
    "layer_dims = [34, 64, 16, 1]\n",
    "epoches = 50000  # Reduced from 50000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "learning_rate = 0.0008\n",
    "lambda_val = 0.02  # Add L2 regularization strength\n",
    "keep_prob = 0.4    # Apply dropout to hidden layers\n",
    "\n",
    "weights, biases = train(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    layer_dims, \n",
    "    epoches, \n",
    "    learning_rate, \n",
    "    train_losses, \n",
    "    X_test, \n",
    "    Y_test, \n",
    "    test_losses,\n",
    "    seed=42,  # For reproducibility\n",
    "    keep_prob=keep_prob,\n",
    "    lambda_val=lambda_val\n",
    ")\n",
    "save_parameters(weights, biases, index_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good or Bad - Binary Cross Entropy Loss on Train Set: 2.468370104127634\n",
      "Good or Bad - Binary Cross Entropy Loss on Test Set: 3.5077052124090065\n",
      "RMSE on Train Set: 0.25583247913870905\n",
      "RMSE on Test Set: 0.33963593906299816\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Root Mean Squared Error (RMSE) function\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Step 5: Predict\n",
    "_, A_test, _ = forward_propagation(X_test, weights, biases)\n",
    "_, A_train, _ = forward_propagation(X_train, weights, biases)\n",
    "\n",
    "Y_pred_test = A_test[f\"A{len(layer_dims)-1}\"]\n",
    "Y_pred_train = A_train[f\"A{len(layer_dims)-1}\"]\n",
    "\n",
    "# Binarize predictions for classification\n",
    "Y_pred_train_bin = np.where(Y_pred_train < 0.5, 0, 1)\n",
    "Y_pred_test_bin = np.where(Y_pred_test < 0.5, 0, 1)\n",
    "\n",
    "# Evaluate the performance for TRAIN using Binary Cross-Entropy Loss on the Training Set\n",
    "bce_loss_train = binary_cross_entropy_loss(Y_train, Y_pred_train_bin, weights)  # Binary predictions for BCE\n",
    "bce_loss_test = binary_cross_entropy_loss(Y_test, Y_pred_test_bin, weights)  # Binary predictions for BCE\n",
    "\n",
    "# Calculate RMSE for Train and Test\n",
    "rmse_train = rmse_loss(Y_train, Y_pred_train)  # RMSE for predicted probabilities\n",
    "rmse_test = rmse_loss(Y_test, Y_pred_test)  # RMSE for predicted probabilities\n",
    "\n",
    "# Print the results\n",
    "print(f\"Good or Bad - Binary Cross Entropy Loss on Train Set: {bce_loss_train}\")\n",
    "print(f\"Good or Bad - Binary Cross Entropy Loss on Test Set: {bce_loss_test}\")\n",
    "print(f\"RMSE on Train Set: {rmse_train}\")\n",
    "print(f\"RMSE on Test Set: {rmse_test}\")\n",
    "\n",
    "# Visualize the learning curve (TRAIN DATA and TEST DATA combined)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.title(\"Learning Curves - Train vs Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()  # Show legend to differentiate between train and test curves\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
